# run_experiment_cube.py
import os
import csv
import psycopg2
from datetime import datetime
import matplotlib.pyplot as plt
import numpy as np

from config_cube import (
    db_config,
    point_csv,
    log_dir,
    image_dir,
    query_table,
    topKs,
    groundtruth_table,
    work_mem,
    maintenance_work_mem,
    gist_fillfactor,
    gist_index_scan_cost,
)

SCHEMA = "public"


def log(msg: str):
    now = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
    print(f"{now} {msg}")


def ensure_dir(path: str):
    if not os.path.exists(path):
        os.makedirs(path)


def read_header_and_dim(csv_path: str):
    with open(csv_path, newline="") as f:
        reader = csv.reader(f)
        header = next(reader, None)
    if not header or len(header) < 2:
        raise ValueError(f"CSV header ä¸åˆæ³•ï¼ˆè‡³å°‘ id+1ç»´ï¼‰ï¼š{csv_path}")
    if header[0].lower() != "id":
        raise ValueError(f"æŸ¥è¯¢ CSV ç¬¬ä¸€åˆ—å¿…é¡»æ˜¯ idï¼Œå½“å‰ä¸º {header[0]}ï¼š{csv_path}")
    return header, len(header) - 1


def load_query_vectors(file_path: str):
    """
    point_csv æ ¼å¼ï¼š
      id, feature_1, feature_2, ..., feature_d
    d åŠ¨æ€æ¨æ–­
    """
    header, dim = read_header_and_dim(file_path)
    log(f"ğŸ”¹ åŠ è½½æŸ¥è¯¢å‘é‡: {file_path} | dim={dim}")
    vectors = []

    with open(file_path, newline="") as f:
        reader = csv.reader(f)
        next(reader, None)  # skip header
        for row_idx, row in enumerate(reader):
            if not row:
                continue
            vec_part = row[1:1 + dim]
            if len(vec_part) != dim:
                raise ValueError(
                    f"ç¬¬ {row_idx} è¡Œç»´åº¦ä¸ä¸€è‡´ï¼šæœŸæœ› {dim}ï¼Œå®é™… {len(vec_part)}"
                )
            vectors.append(list(map(float, vec_part)))

    log(f"  -> åŠ è½½å®Œæˆï¼Œå…± {len(vectors)} ä¸ªæŸ¥è¯¢å‘é‡")
    return vectors, dim


def compute_recall(result_ids, gt_ids):
    if not gt_ids:
        return 0.0
    return len(set(result_ids) & set(gt_ids)) / len(gt_ids)


def find_groundtruth_for_table(q_table: str):
    """
    åœ¨ groundtruth_table ä¸­æ‰¾åˆ° q_table å±äºå“ªä¸ª gt
    """
    for gt, tables in groundtruth_table.items():
        if q_table in tables:
            return gt
    return None


def get_first_col_name_and_value(table_name: str):
    """
    åªå¯¹ _gist è¡¨å†™ KNN CSV
    ç¬¬ä¸€åˆ—å†™ fillfactorï¼ˆæ„å»ºå‚æ•°ï¼‰
    """
    if table_name.endswith("_gist"):
        return "fillfactor", int(gist_fillfactor)
    return None, None


def get_cube_dim_from_db(cursor, table_name: str) -> int:
    """
    cube ç»´åº¦ï¼šcube_dim(vec)
    """
    cursor.execute(f'SELECT cube_dim(vec) FROM "{SCHEMA}"."{table_name}" LIMIT 1;')
    r = cursor.fetchone()
    if not r or r[0] is None:
        raise ValueError(
            f"æ— æ³•ä»è¡¨ {SCHEMA}.{table_name} æ¨æ–­ cube_dim(vec)ï¼šè¡¨ä¸ºç©ºæˆ– vec åˆ—å¼‚å¸¸"
        )
    return int(r[0])


def explain_analyze(cursor, sql: str):
    cursor.execute("EXPLAIN ANALYZE " + sql)
    rows = cursor.fetchall()
    plan_lines = [r[0] for r in rows]
    exec_time = -1.0
    for line in plan_lines:
        if "Execution Time" in line:
            # e.g. "Execution Time: 0.123 ms"
            exec_time = float(line.split(":")[1].strip().split()[0])
            break
    return plan_lines, exec_time


def execute_knn_query(cursor, table: str, vec, k: int):
    """
    cube KNN æŸ¥è¯¢ï¼š
      ORDER BY vec <-> cube(ARRAY[...]) LIMIT k
    """
    arr = "ARRAY[" + ",".join(map(str, vec)) + "]"
    sql = f'''
        SELECT id
        FROM "{SCHEMA}"."{table}"
        ORDER BY vec <-> cube({arr})
        LIMIT {k}
    '''
    # EXPLAIN ANALYZE
    plan_lines, exec_time = explain_analyze(cursor, sql.strip())

    # å®é™…æŸ¥è¯¢æ‹¿ id
    cursor.execute(sql)
    ids = [r[0] for r in cursor.fetchall()]
    return ids, exec_time, plan_lines


def run():
    log("=== ğŸš€ cube + GiST å®éªŒå¯åŠ¨ï¼ˆåŠ¨æ€ç»´åº¦ï¼›ä»… _gist å†™ KNN CSVï¼›å†™å…¥ plansï¼‰ ===")
    ensure_dir(log_dir)
    ensure_dir(image_dir)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    cur_log_dir = os.path.join(log_dir, timestamp)
    cur_img_dir = os.path.join(image_dir, timestamp)
    cur_plan_dir = os.path.join(cur_log_dir, "plans")
    ensure_dir(cur_log_dir)
    ensure_dir(cur_img_dir)
    ensure_dir(cur_plan_dir)

    log(f"ğŸ“ æ—¥å¿—è·¯å¾„: {cur_log_dir}")
    log(f"ğŸ–¼ï¸ å›¾ç‰‡è·¯å¾„: {cur_img_dir}")
    log(f"ğŸ§  EXPLAIN plans: {cur_plan_dir}")

    vectors, q_dim = load_query_vectors(point_csv)

    conn = psycopg2.connect(**db_config)
    cursor = conn.cursor()

    # cube æ‰©å±•
    cursor.execute("CREATE EXTENSION IF NOT EXISTS cube;")
    conn.commit()

    # session å‚æ•°ï¼ˆæ¯æ¬¡æŸ¥è¯¢å‰ä¹Ÿä¼š set ä¸€æ¬¡ï¼Œè¿™é‡Œå…ˆ set ä¸€æ¬¡ï¼‰
    cursor.execute(f"SET work_mem = '{work_mem}';")
    cursor.execute(f"SET maintenance_work_mem = '{maintenance_work_mem}';")
    cursor.execute(f"SET cpu_index_tuple_cost = {float(gist_index_scan_cost)};")

    # âœ… ç»´åº¦æ ¡éªŒï¼šå¯¹æ¯ä¸ª query_table å¯¹åº”çš„ gt è¡¨æ£€æŸ¥ä¸€è‡´æ€§
    for t in query_table:
        gt = find_groundtruth_for_table(t)
        if gt is None:
            raise ValueError(
                f"query_table é‡Œçš„è¡¨ {t} æ²¡æœ‰å‡ºç°åœ¨ groundtruth_table çš„ä»»ä½•åˆ—è¡¨é‡Œï¼Œè¯·ä¿®æ­£é…ç½®ã€‚"
            )
        db_dim = get_cube_dim_from_db(cursor, gt)
        log(f"ğŸ” ç»´åº¦æ ¡éªŒï¼štable={t} gt={gt} | query_dim={q_dim}, db_dim={db_dim}")
        if q_dim != db_dim:
            raise ValueError(
                f"ç»´åº¦ä¸ä¸€è‡´ï¼špoint_csv={q_dim}ç»´ï¼Œä½† groundtruthè¡¨ {SCHEMA}.{gt} çš„ vec æ˜¯ {db_dim} ç»´ã€‚"
            )

    for k in topKs:
        log(f"\n=== ğŸ” Top{k} å¼€å§‹ ===")

        recalls_per_table = {t: [] for t in query_table}
        times_per_table = {t: [] for t in query_table}

        detail_txt = os.path.join(cur_log_dir, f"top{k}_details.txt")
        knn_csv = os.path.join(cur_log_dir, f"top{k}_knn_ids.csv")

        with open(detail_txt, "w", encoding="utf-8") as dlog, open(knn_csv, "w", newline="", encoding="utf-8") as cfile:
            dlog.write("QueryIndex,Table,ExecTime(ms),Recall\n")
            writer = csv.writer(cfile)
            writer.writerow(["fillfactor", "k", "query", "id_list"])

            for qi, vec in enumerate(vectors):
                log("\n" + "-" * 70)
                log(f"â¡ï¸ query={qi}ï¼ˆ0-basedï¼‰TopK={k}")

                for t in query_table:
                    gt = find_groundtruth_for_table(t)

                    try:
                        # æ¯æ¬¡æŸ¥è¯¢å‰è®¾ç½® session å‚æ•°ï¼ˆä¿æŒä¸€è‡´æ€§ï¼‰
                        cursor.execute(f"SET work_mem = '{work_mem}';")
                        cursor.execute(f"SET maintenance_work_mem = '{maintenance_work_mem}';")
                        cursor.execute(f"SET cpu_index_tuple_cost = {float(gist_index_scan_cost)};")

                        gt_ids, _, gt_plan = execute_knn_query(cursor, gt, vec, k)
                        ids, ms, plan = execute_knn_query(cursor, t, vec, k)

                        # å†™ planï¼ˆæ¯æ¡æŸ¥è¯¢ä¸€ä¸ªæ–‡ä»¶ï¼Œä¾¿äºä½ ç¡®è®¤æ˜¯å¦ Index Scanï¼‰
                        plan_path = os.path.join(cur_plan_dir, f"q{qi}_k{k}_{t}.plan.txt")
                        with open(plan_path, "w", encoding="utf-8") as pf:
                            pf.write("\n".join(plan))

                    except Exception as e:
                        log(f"âŒ æŸ¥è¯¢å¤±è´¥ table={t}: {e}")
                        cursor.execute("ROLLBACK;")
                        continue

                    recall = compute_recall(ids, gt_ids)

                    log(f"ğŸ“Œ {t} (gt={gt}) â±ï¸ {ms:.2f} ms | recall={recall:.4f}")
                    recalls_per_table[t].append(recall)
                    times_per_table[t].append(ms)
                    dlog.write(f"{qi},{t},{ms:.2f},{recall:.4f}\n")

                    # âœ… ä»… _gist è¡¨å†™ KNN CSV
                    col, val = get_first_col_name_and_value(t)
                    if col is not None:
                        writer.writerow([val, k, qi, " ".join(map(str, ids))])

        log(f"ğŸ§¾ Top{k} KNN CSVï¼ˆä»… _gist è¡¨ï¼‰: {knn_csv}")

        # summary md
        summary_md = os.path.join(cur_log_dir, f"top{k}_summary.md")
        with open(summary_md, "w", encoding="utf-8") as s:
            s.write(f"# Top{k} æŸ¥è¯¢æ±‡æ€»ï¼ˆcube+GiSTï¼‰\n\n")
            s.write("| Table | Total Time (ms) | Avg Time (ms) | Avg Recall |\n")
            s.write("|-------|----------------|----------------|-------------|\n")
            for t in query_table:
                total = sum(times_per_table[t])
                avg = total / len(times_per_table[t]) if times_per_table[t] else 0.0
                r = sum(recalls_per_table[t]) / len(recalls_per_table[t]) if recalls_per_table[t] else 0.0
                s.write(f"| {t} | {total:.2f} | {avg:.2f} | {r:.4f} |\n")

        # plot
        try:
            x = np.arange(len(query_table))
            totals = [sum(times_per_table[t]) for t in query_table]
            ravg = [
                (sum(recalls_per_table[t]) / len(recalls_per_table[t])) if recalls_per_table[t] else 0.0
                for t in query_table
            ]

            fig, ax1 = plt.subplots(figsize=(10, 5))
            ax1.bar(x, totals)
            ax1.set_ylabel("Total Query Time (ms)")

            ax2 = ax1.twinx()
            ax2.plot(x, ravg, marker="o")
            ax2.set_ylabel("Average Recall")

            plt.xticks(x, query_table, rotation=20, ha="right")
            plt.title(f"Top{k} (cube+GiST)")
            fig.tight_layout()

            out_png = os.path.join(cur_img_dir, f"top{k}_summary.png")
            plt.savefig(out_png)
            plt.close()
            log(f"ğŸ“Š å›¾ç‰‡ä¿å­˜: {out_png}")
        except Exception as e:
            log(f"âš ï¸ ç»˜å›¾å¤±è´¥: {e}")

    cursor.close()
    conn.close()
    log("âœ… å®Œæˆ")
    log(f"ğŸ“ æ—¥å¿—ç›®å½•: {cur_log_dir}")
    log(f"ğŸ§  Plan ç›®å½•: {cur_plan_dir}")
    log(f"ğŸ–¼ï¸ å›¾ç‰‡ç›®å½•: {cur_img_dir}")


if __name__ == "__main__":
    run()
